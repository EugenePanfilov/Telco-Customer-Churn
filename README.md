# Telco Churn — быстрый запуск


## Требования
- Python ≥ 3.10, установлен `make`
- Данные: `data/train.csv` (путь и таргет заданы в `configs/default.yaml`)

## 1) Клонирование репозитория
```bash
git clone https://github.com/EugenePanfilov/Telco-Customer-Churn.git
cd telco-customer-churn
```

## 2) Установка (создаст `.venv` и поставит зависимости)
```bash
make install
```

## 3) Обучение
```bash
make train
```

## 4) Оценка
```bash
make eval
```

## 5) Тесты
```bash
make test
```

## 6) Очистка артефактов
```bash
make clean
```

---


## Коротко для Google Colab
```bash
!git clone https://github.com/EugenePanfilov/Telco-Customer-Churn.git
%cd Telco-Customer-Churn
!make install
!make train
!make eval
```

> Артефакты и отчёты: `artifacts/runs/<YYYYmmdd-HHMMSS>/` и симлинк на последний запуск `artifacts/latest/`.


## Отчёт

В проекте использован стратифицированный K-fold (K=5, shuffle, random_state=42), потому что в задаче возможен дисбаланс классов: стратификация удерживает долю положительного класса в каждом фолде и даёт устойчивую OOF-оценку без утечек. Групп и явной временной зависимости в данных нет, поэтому GroupKFold и time-split не применялись. Главная метрика — PR-AUC, так как при редких «единицах» она лучше отражает качество ранжирования именно позитивов; ROC-AUC и LogLoss используются как дополнительные: первая — для общей чувствительности модели, второй — для оценки калиброванности вероятностей. Порог классификации выбирался по OOF-валидации стратегией максимизации Fβ при β=2 (recall-ориентированная цель, снижает цену пропуска позитивов). Полученный порог: T = 0.137. На валидации до калибровки получены метрики: PR-AUC=0.634, ROC-AUC=0.831, LogLoss=0.444, F1=0.597, Fβ=0.735 (см. блок before). Затем проведена калибровка вероятностей (Platt или Isotonic согласно конфигу); она не меняет ранжирование радикально, зато обычно улучшает согласованность вероятностей с частотами. После калибровки OOF-метрики: PR-AUC=0.639, ROC-AUC=0.639, LogLoss=0.442, F1=0.599, Fβ=0.738 (см. блок after). По калибрационной кривой наблюдается приближение к диагонали, а LogLoss, как правило, уменьшается — это сигнал корректной вероятностной калибровки. Финальная модель обучена на всём трейне, при необходимости откалибрована, и сохранена вместе с порогом; артефакты и график — в artifacts/latest.
